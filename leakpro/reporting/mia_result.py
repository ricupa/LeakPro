"""Contains the Result classes for MIA, MiNVA, and GIA attacks."""

import json
import os

import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import auc

from leakpro.reporting.report_utils import get_config_name, reduce_to_unique_labels
from leakpro.schemas import MIAResultSchema
from leakpro.utils.import_helper import Any, Self, Union
from leakpro.utils.logger import logger


class MIAResult:
    """Contains results related to the performance of the metric."""

    def __init__(self, *args, **kwargs) -> None:  # noqa: ANN002, ANN003, ARG002
        raise RuntimeError(
            "Use one of the constructors: "
            "from_full_scores(), from_fixed_thresholds(), or from_confusion_counts()"
        )

    @classmethod
    def from_full_scores(cls,
                         true_membership:list,
                         signal_values:list,
                         result_name:str=None,
                         metadata:dict=None) -> Self:
        """Create MIAResult from full scores.

        Args:
        ----
            true_membership: True membership labels used to evaluate the metrics.
            signal_values: Signal values generated by the attack. Large signal values are indicative of membership.
            metadata: Metadata about the results
            result_name: The name of the attack and result

        Returns:
        -------
            MIAResult: An instance of MIAResult with the computed metrics.

        """
        if metadata is None:
            metadata = {}
        obj = object.__new__(cls)
        obj._init_common(true_membership, result_name, metadata)
        obj.roc_mode = "full"
        obj.signal_values = np.ravel(signal_values)
        obj._compute_confusion_arrays()
        obj._compute_metrics()
        obj.result = obj._make_result_object()
        return obj

    @classmethod
    def from_fixed_thresholds(cls,
                              true_membership: list,
                              signal_values: list,
                              thresholds: list,
                              result_name: str = None,
                              metadata: dict = None) -> Self:
        """Create MIAResult from fixed threshold scores.

        Args:
        ----
            true_membership: True membership labels.
            signal_values: Signal values output by the attack.
            thresholds: A list of fixed thresholds at which to evaluate confusion values.
            result_name: The name of the attack and result.
            metadata: Metadata about the result.

        Returns:
        -------
            MIAResult: An instance with metrics based on fixed thresholds.

        """
        if metadata is None:
            metadata = {}
        obj = object.__new__(cls)
        obj._init_common(true_membership, result_name, metadata)
        obj.roc_mode = "fixed"
        obj.signal_values = np.ravel(signal_values)
        obj._compute_fixed_threshold_confusions(thresholds)
        obj._compute_metrics()
        obj.result = obj._make_result_object()
        return obj

    @classmethod
    def from_confusion_counts(cls,
                              true_membership: list,
                              tp: Union[list[int], int],
                              fp: Union[list[int], int],
                              tn: Union[list[int], int],
                              fn: Union[list[int], int],
                              result_name: str = None,
                              metadata: dict = None) -> Self:
        """Create MIAResult directly from precomputed confusion values.

        Args:
        ----
            true_membership: True membership labels (used only to compute accuracy).
            tp: Number of true positives.
            fp: Number of false positives.
            tn: Number of true negatives.
            fn: Number of false negatives.
            result_name: The name of the attack and result.
            metadata: Metadata about the result.

        Returns:
        -------
            MIAResult: An instance with metrics based on given confusion values.

        """
        if metadata is None:
            metadata = {}
        obj = object.__new__(cls)
        obj._init_common(true_membership, result_name, metadata)
        obj.signal_values = None
        obj.tp = np.atleast_1d(tp)
        obj.fp = np.atleast_1d(fp)
        obj.tn = np.atleast_1d(tn)
        obj.fn = np.atleast_1d(fn)

        if len(obj.tp) == 1:
            obj.roc_mode = "none"
        else:
            obj.roc_mode = "fixed"

        obj._compute_metrics()
        obj.result = obj._make_result_object()
        return obj

    def _init_common(self, true_membership: list, result_name: str, metadata: dict) -> None:
        """Initialize common attributes."""
        self.true = np.ravel(true_membership)
        self.result_name = result_name
        self.metadata = metadata
        result_config = self._safe_model_dump(self.metadata)
        readable_name = get_config_name(result_config) # concatenate config params to name
        self.id = f"{self.result_name}{readable_name}".replace("/", "__")

    def _make_result_object(self) -> MIAResultSchema:
        return MIAResultSchema(
            result_name = self.result_name,
            result_type = self.__class__.__name__,
            id = self.id,
            tpr = self.tpr,
            fpr = self.fpr,
            roc_auc = self.roc_auc,
            accuracy = self.accuracy,
            config = self.metadata,
            fixed_fpr = self.fixed_fpr_table,
            signal_values = self.signal_values,
            true_labels = self.true,
            tp = self.tp,
            fp = self.fp,
            tn = self.tn,
            fn = self.fn,
        )

    def _compute_confusion_arrays(self) -> None:
        """Compute confusion arrays for a full ROC sweep from signal values."""

        # Sort signal_values in descending order and align labels
        sorted_indices = np.argsort(-self.signal_values)
        sorted_labels = self.true[sorted_indices]
        sorted_scores = self.signal_values[sorted_indices]

        assert np.all(np.diff(sorted_scores) <= 0), "sorted_scores are not in descending order"

        # check that sorted_scores are descending
        assert np.all(np.diff(sorted_scores) <= 0), "sorted_scores are not in descending order"

        tp_cumsum = np.cumsum(sorted_labels == 1)
        fp_cumsum = np.cumsum(sorted_labels == 0)

        # Remove duplicates in sorted_scores and keep the first occurrence
        _, first_indices = np.unique(sorted_scores, return_index=True)
        first_indices = first_indices[::-1]

        self.tp = tp_cumsum[first_indices]
        self.fp = fp_cumsum[first_indices]
        self.thresholds = sorted_scores[first_indices]

        self.fn = np.sum(sorted_labels == 1) - self.tp
        self.tn = np.sum(sorted_labels == 0) - self.fp

        assert np.all(self.tp + self.fp + self.tn + self.fn == len(self.true)), "Confusion counts do not sum to total samples"

        if len(self.tp) == 1:
            self.roc_mode = "none"

    def _compute_fixed_threshold_confusions(self, thresholds: list) -> None:
        """Compute confusion values at fixed thresholds."""
        signal_values = self.signal_values
        labels = self.true

        tp_list, fp_list, tn_list, fn_list = [], [], [], []

        for thresh in thresholds:
            preds = (signal_values >= thresh).astype(int)
            tp = np.sum((preds == 1) & (labels == 1))
            fp = np.sum((preds == 1) & (labels == 0))
            tn = np.sum((preds == 0) & (labels == 0))
            fn = np.sum((preds == 0) & (labels == 1))

            tp_list.append(tp)
            fp_list.append(fp)
            tn_list.append(tn)
            fn_list.append(fn)

        self.tp = np.array(tp_list)
        self.fp = np.array(fp_list)
        self.tn = np.array(tn_list)
        self.fn = np.array(fn_list)

    def _compute_metrics(self) -> None:
        """Compute evaluation metrics such as accuracy, ROC AUC, and FPR/TPR."""
        total = self.tp + self.fp + self.tn + self.fn
        self.accuracy = (self.tp + self.tn) / np.maximum(total, 1)

        if self.roc_mode == "none":
            self.fpr = None
            self.tpr = None
            self.roc_auc = None
            self.fixed_fpr_table = None
            return

        self.fpr = np.where(self.fp + self.tn != 0, self.fp / (self.fp + self.tn), 0.0)
        self.tpr = np.where(self.tp + self.fn != 0, self.tp / (self.tp + self.fn), 0.0)

        if np.sum(np.diff(self.fpr) < 0) > 0 or np.sum(np.diff(self.tpr) < 0) > 0:
            logger.warning("FPR or TPR values are not monotonically increasing. ROC AUC may be inaccurate.")
            self.roc_auc = None
            self.fixed_fpr_table = None
            return

        self.roc_auc = auc(self.fpr, self.tpr)
        self.fixed_fpr_table = self._get_result_fixed_fpr([0.0, 0.0001, 0.001, 0.01, 0.1])

    def _get_result_fixed_fpr(self, fpr_targets: list[float]) -> dict[str, float]:
        """Return TPR values at selected FPR thresholds."""

        def format_percent(f: float) -> str:
            percentage = f * 100
            # Format with enough precision and strip trailing zeros
            formatted = f"{percentage:.8f}".rstrip("0").rstrip(".")
            return f"TPR@{formatted}%FPR"

        return {
            format_percent(fpr_target): max(
                (tpr for fpr, tpr in zip(self.fpr, self.tpr) if fpr <= fpr_target),
                default=0.0
            ) for fpr_target in fpr_targets
        }

    def _get_roc_auc_in_fpr_interval(self:Self, ub:float) -> float:
        """Calculate the average TPR for FPR values below fpr_threshold."""

        assert ub > 0.0, "Upper bound must be greater than 0.0"
        assert ub < 1.0, "Upper bound must be less than 1.0"

        if self.roc_mode == "none":
            logger.warning("ROC mode is 'none', cannot compute ROC AUC in FPR interval, returning zero.")
            return 0.0
        return float(np.mean(self.tpr[self.fpr < ub]))

    def _safe_model_dump(self, obj:object) -> dict:
        """Safely dump the object to a dictionary."""
        if isinstance(obj, dict):
            return obj
        if hasattr(obj, "model_dump"):
            return obj.model_dump()
        return {}

    def _create_dir(self, path: str) -> None:
        if not os.path.exists(path):
            os.makedirs(path)

    def _has_roc(self) -> bool:
        """Return True if this result supports a meaningful ROC curve."""
        return self.roc_mode != "none" and self.fpr is not None and len(self.fpr) > 1

    def save(self:Self, attack_obj: Any, output_dir: str) -> None:
        """Save the MIAResults to disk."""

        attack_name = attack_obj.__class__.__name__.lower()
        save_path = f"{output_dir}/results/{self.id}"
        self._create_dir(save_path)

        # Create directory for saving data objects
        data_obj_storage = f"{output_dir}/data_objects/"
        self._create_dir(data_obj_storage)

        def json_fallback(obj: Any) -> Any:
            """Fallback function for JSON serialization."""
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            if isinstance(obj, (np.integer, np.floating)):
                return obj.item()
            raise TypeError(f"Object of type {type(obj)} is not JSON serializable")

        # Save the results to a file in data objects using attack hash
        with open(f"{data_obj_storage}{attack_obj.attack_id}.json", "w") as f:
            json.dump(self.result.model_dump(), f, default=json_fallback)

        # Store results for user output
        with open(f"{save_path}/result.txt", "w") as f:
            f.write(str(self.result.model_dump()))

        # Create ROC plot
        if self._has_roc():
            logger.info(f"Creating ROC plot for {attack_name}")
            self.create_roc_plot([self], save_dir = save_path)

        # Create SignalHistogram plot for MIAResult
        if self.signal_values is not None:
            logger.info(f"Creating SignalHistogram plot for {attack_name}")
            self.create_signal_histogram(save_path = save_path)

    def create_signal_histogram(self:Self, save_path: str) -> None:
        """Method to create Signal Histogram."""

        filename = f"{save_path}/SignalHistogram"
        values = np.array(self.signal_values).ravel()
        labels = np.array(self.true).ravel()

        # Split values by membership
        member_values = values[labels == 1]
        non_member_values = values[labels == 0]

        # Compute bin edges (shared for both histograms)
        bin_edges = np.histogram_bin_edges(values, bins=1000)

        # Plot histograms
        plt.hist(non_member_values, bins=bin_edges, histtype="step", label="out-member", density=False)
        plt.hist(member_values, bins=bin_edges, histtype="step", label="in-member", density=False)

        plt.grid()
        plt.xlabel("Signal value")
        plt.ylabel("Number of samples")
        plt.title("Signal histogram")
        plt.legend()
        plt.tight_layout()
        plt.savefig(fname=filename + ".png", dpi=1000)
        plt.clf()

    @classmethod
    def load(cls, data_path:str) -> Self:
        """Load MIAResults from disk.

        Args:
        ----
            data_path: Path to the JSON file containing the MIAResults.

        Returns:
        -------
            MIAResult: An instance of MIAResult with the loaded data.

        """
        with open(data_path, "r") as f:
            mia_data = json.load(f)
        mia_data = MIAResultSchema(**mia_data) # validate and parse the data

        # Create a new instance of MIAResult
        obj = object.__new__(cls)

        obj.result_name = mia_data.result_name
        obj.id = mia_data.id
        obj.tpr = mia_data.tpr if mia_data.tpr else None
        obj.fpr = mia_data.fpr if mia_data.fpr else None
        obj.roc_auc = mia_data.roc_auc if mia_data.roc_auc else None
        obj.accuracy = mia_data.accuracy
        obj.metadata = mia_data.config
        obj.fixed_fpr_table = mia_data.fixed_fpr if mia_data.fixed_fpr else None
        obj.signal_values = mia_data.signal_values if mia_data.signal_values else None
        obj.true = mia_data.true_labels
        obj.tp = mia_data.tp
        obj.fp = mia_data.fp
        obj.tn = mia_data.tn
        obj.fn = mia_data.fn

        return obj


    @staticmethod
    def create_roc_plot(result_objects:list, save_dir: str) -> None:
        """Plot method for MIAResult. This method can be used by individual result objects or multiple.

        Args:
        ----
            result_objects (list): List of MIAResult objects to plot.
            save_dir (str): Directory to save the plot.

        """

        filename = f"{save_dir}/ROC"

        # Create plot for results
        assert isinstance(result_objects, list), "Results must be a list of MIAResult objects"

        reduced_labels = reduce_to_unique_labels(result_objects)
        for res, label in zip(result_objects, reduced_labels):
            plt.fill_between(res.fpr, res.tpr, alpha=0.15)
            plt.plot(res.fpr, res.tpr, label=label)

        # Plot baseline (random guess)
        range01 = np.linspace(0, 1)
        plt.plot(range01, range01, "--", label="Random guess")

        # Set plot parameters
        plt.yscale("log")
        plt.xscale("log")
        plt.xlim(left=1e-5)
        plt.ylim(bottom=1e-5)
        plt.tight_layout()
        plt.grid()
        plt.legend(bbox_to_anchor =(0.5,-0.27), loc="lower center")

        plt.xlabel("False positive rate (FPR)")
        plt.ylabel("True positive rate (TPR)")
        plt.title("ROC Curve")
        plt.savefig(fname=f"{filename}.png", dpi=1000, bbox_inches="tight")
        plt.clf()

    @staticmethod
    def create_results(
            results: list,
            save_dir: str = "./",
        ) -> str:
        """Result method for MIAResult."""
        # create folder if it does not exist
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)

        latex = ""

        seen_ids = set()
        attack_results = [result for result in results if not (result.id in seen_ids or seen_ids.add(result.id))]
        # Create plot for all results
        MIAResult.create_roc_plot(attack_results, save_dir)
        latex += MIAResult._latex(attack_results, save_dir = save_dir, section_title = "All results")

        # Create individual plot for results
        for res in attack_results:
            attack_folder = f"{save_dir}/{res.id}"
            if not os.path.exists(attack_folder):
                os.makedirs(attack_folder)
            MIAResult.create_roc_plot([res], attack_folder)
            latex += MIAResult._latex([res], save_dir = attack_folder, section_title = res.id)

        return latex

    @staticmethod
    def _latex(
            results: list,
            save_dir: str, # noqa: ARG004
            section_title: str
        ) -> str:
        """Latex method for MIAResult."""
        image_path = os.path.abspath(os.path.join(save_dir, "ROC.png")).replace("_", "\\_")

        # Input mia results image
        latex_content = f"""
        \\subsection{{{" ".join(section_title.split("_"))}}}
        \\begin{{figure}}[ht]
        \\centering
        \\includegraphics[width=0.8\\textwidth]{{{image_path}}}
        \\end{{figure}}
        """

        # Initialize latex table
        latex_content += """
        \\resizebox{\\linewidth}{!}{%
        \\begin{tabularx}{\\textwidth}{l c l l l l}
        Attack name & attack config & TPR: 10.0\\%FPR & 1.0\\%FPR & 0.1\\%FPR & 0.0\\%FPR \\\\ \\hline """ # noqa: W291

        # Convert config to latex table input
        def config_latex_style(config: str) -> str:
            config = " \\\\ ".join(config.split("-")[1:])
            config = "-".join(config.split("_"))
            return f"""\\shortstack{{{config}}}"""

        # Append all mia results to table
        for res in results:
            config = config_latex_style(res.id)
            latex_content += f"""
            {"-".join(res.result_name.split("_"))} & {config} & {res.fixed_fpr_table["TPR@10%FPR"]} & {res.fixed_fpr_table["TPR@1%FPR"]} & {res.fixed_fpr_table["TPR@0.1%FPR"]} & {res.fixed_fpr_table["TPR@0%FPR"]} \\\\ \\hline """ # noqa: E501
        latex_content += """
        \\end{tabularx}
        }
        \\newline
        """
        return latex_content
